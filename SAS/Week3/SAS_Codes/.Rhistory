for item in full_url_list[:5]: print(item)
quit
# Step 2: Create a dataframe from the list created in the previous step
import pandas as pd
import numpy as np
col1 = "full_url"
df = pd.DataFrame({col1:full_url_list})
values = ['v9.zip', 'ssp.zip', 'dta.zip', 'dat.zip', 'xlsx.zip']
sub_string = list(map(df['full_url'].str.contains, values))
df['sub_string'] = np.select(sub_string, values, 'other')
print('There are', f"{len(df):,}", 'Full URLs for data files with extensions of interest.')
df['sub_string'].value_counts()
# Step 3: Create an Excel Workbook with multiple sheets
import pandas as pd
import xlsxwriter
with pd.ExcelWriter('C:/Data/MEPS_urls_workbook.xlsx') as writer:
for i, x in df.groupby('sub_string'):
x.drop('sub_string', axis=1).to_excel(writer, sheet_name=i, index=False)
#| echo: false
# Step 1: List all MEPS data file URLs
import requests
from bs4 import BeautifulSoup
full_url_list = []
tuple_values = 'v9.zip', 'ssp.zip', 'dta.zip', 'dat.zip', 'xlsx.zip', '/'
def get_links(base_link):
response = requests.get(base_link)
soup = BeautifulSoup(response.text, 'html.parser')
tags = soup.find_all('a')
for tag in tags:
if tag.text.endswith(tuple_values):
href = tag.get_text()
full_url = base_link + href
if href[-1]=='/':
get_links(full_url)
else:
full_url_list.append(full_url)
get_links('https://meps.ahrq.gov/data_files/pufs/')
print('There are', f"{len(full_url_list):,}", 'Full URLs for 5 format-specific ZIP data files on the MEPS Website.')
# loop until the limit or the end of the list, whichever occurs first.
print('Listing of first 5 URLs')
for item in full_url_list[:5]: print(item)
quit
quit
#| echo: false
library(reticulate)
py_install("pandas", "numpy")
reticulate::repl_python()
# Step 2: Create a dataframe from the list created in the previous step
import pandas as pd
import numpy as np
col1 = "full_url"
df = pd.DataFrame({col1:full_url_list})
values = ['v9.zip', 'ssp.zip', 'dta.zip', 'dat.zip', 'xlsx.zip']
sub_string = list(map(df['full_url'].str.contains, values))
df['sub_string'] = np.select(sub_string, values, 'other')
print('There are', f"{len(df):,}", 'Full URLs for data files with extensions of interest.')
df['sub_string'].value_counts()
# Step 3: Create an Excel Workbook with multiple sheets
import pandas as pd
import xlsxwriter
with pd.ExcelWriter('C:/Data/MEPS_urls_workbook.xlsx') as writer:
for i, x in df.groupby('sub_string'):
x.drop('sub_string', axis=1).to_excel(writer, sheet_name=i, index=False)
reticulate::repl_python()
fd = open('C:\sascourse\week14\SAS_Codes\TV_Data_noheader.csv')
print(fd.read())
fd.close()
fd = open('C:\Data\SAS_MEPS_web')
print(fd.read())
fd.close()
fd = open(r"C:\Data\SAS_MEPS_web")
print(fd.read())
fd.close()
fd = open(r"C:\Data\SAS_MEPS_web.sas")
print(fd.read())
fd.close()
fd = open(r"C:\Data\SAS_MEPS_web.sas")
print(fd.read())
fd.close()
import pandas as pd
df_exl = pd.read_excel('C:\\Data\\class.xlsx', sheet_name='Sheet',header=TRUE)
print(df_exl)
import pandas as pd
df_exl = pd.read_excel('C:\\Data\\class.xlsx', sheet_name='Sheet1',header=TRUE)
print(df_exl)
import pandas as pd
df_exl = pd.read_excel('C:\\Data\\class.xlsx', sheet_name='Sheet1')
print(df_exl)
quit
library(reticulate)
py_install("openpyxl")
reticulate::repl_python()
import pandas as pd
df_exl = pd.read_excel('C:\\Data\\class.xlsx', sheet_name='Sheet1')
print(df_exl)
import pandas as pd
df_exl = pd.read_excel('C:\\Data\\class.xlsx', sheet_name='Sheet1')
print(df_exl)
df_exl.to_excel('C:\\Data\\df_to_excel.xlsx')
import pandas as pd
df_exl = pd.read_excel('C:\\Data\\class.xlsx', sheet_name='Sheet1')
print(df_exl)
df_exl.to_excel('C:\\Data\\df_to_excel.xlsx')
df_exl.to_csv('C:\\Data\\df_to_csv.csv', header=TRUE)
import pandas as pd
df_exl = pd.read_excel('C:\\Data\\class.xlsx', sheet_name='Sheet1')
print(df_exl)
df_exl.to_excel('C:\\Data\\df_to_excel.xlsx')
df_exl.to_csv('C:\\Data\\df_to_csv.csv')
reticulate::repl_python()
import os
os.getcwd()
os.chdir(r"C:\Data")
with open("SomeData.txt", "r", encoding='utf-8') as file:
for line in file:
print(line, end="")
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text."format(len(lines)))
fd.close()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
fd.close()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
Myfile.close()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
print(lines[0])
Myfile.close()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
print(lines[0])
for x in range(2:9)
print("{0}: {1}").format(x, lines[x), end=""])
print("Printed above as desired)
Myfile.close()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
print(lines[0])
for x in range(2:9)
print("{0}: {1}").format(x, lines[x], end="")
print("Printed above as desired)
Myfile.close()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
print(lines[0])
for x in range(2:9)
print("{0}: {1}").format(x, lines[x]), end="")
print("Printed above as desired)
Myfile.close()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
print(lines[0])
for x in range(2:9)
print("{0}: {1}").format(x, lines[x]), end="")
print("Printed above as desired")
Myfile.close()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
print(lines[0])
for x in range(2, 9)
print("{0}: {1}").format(x, lines[x]), end="")
print("Printed above as desired")
Myfile.close()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
print(lines[0])
for x in range(2, 9):
print("{0}: {1}").format(x, lines[x]), end="")
print("Printed above as desired")
Myfile.close()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
print(lines[0])
for x in range(2, 9):
print("{0}: {1}".format(x, lines[x]), end="")
print("Printed above as desired")
Myfile.close()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
print(lines[0])
for x in range(2, 10):
print("{0}: {1}".format(x, lines[x]), end="")
print("Printed above as desired")
Myfile.close()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
print(lines[0])
for x in range(1, 10):
print("{0}: {1}".format(x, lines[x]), end="")
print("Printed above as desired")
Myfile.close()
quit
quit
library(reticulate, echo=FALSE)
py_install("openpyxl")
library(reticulate, message = FALSE, warning = FALSE)
library(reticulate)
py_install("openpyxl")
library(reticulate)
py_install("openpyxl")
reticulate::repl_python()
Myfile = open(r"C:\Data\SAS_MEPS_web.sas")
lines = Myfile.readlines()
print("Myfile has {0} lines of text.".format(len(lines)))
print(lines[0])
for x in range(1, 9):
print("{0}: {1}".format(x, lines[x]), end="")
print("Printed above as desired")
Myfile.close()
reticulate::repl_python()
# Step 1: Creating a csv file with MEPS data filenames listed on the web 12/10/2022
import requests
from bs4 import BeautifulSoup, re, Comment
import pandas as pd
import xlsxwriter
# find all options match the start and end string
def extractOptions(inputData):
sub1 = str(re.escape('<option value="All">All data files</option>'))
sub2 = str(re.escape('</select>'))
result = re.findall(sub1+"(.*)"+sub2, inputData, flags=re.S)
if len(result) > 0:
return result[0]
# find the actual data from each option
def extracData(inputData):
sub1 = str(re.escape('>'))
sub2 = str(re.escape('</option>'))
result =  re.findall(sub1+"(.*)"+sub2, inputData, flags=re.S)
if len(result) > 0:
return result[0]
return ''
def main(base_url):
response = requests.get(base_url)
soup = BeautifulSoup(response.text, "html.parser")
comments = soup.find_all(string=lambda text: isinstance(text, Comment))
for c in comments:
if '<select id="pufnumber" size=1 name="cboPufNumber">' in c:
options = extractOptions(c)
ops = options.splitlines() #split text into lines
fp = open(r'C:/Data/MEPS_fn.csv', 'w')
for op in ops:
data = extracData(op)
if data != '': #check if the data found
fp.write(data +'\n')
fp.close()
with open(r'C:/Data/MEPS_fn.csv', 'r') as buff:
for i, line in enumerate(buff, 1):
pass
print('There are', f"{(i)}",  \
'filenames listed in the MEPS Data File Web Page\n')
main('https://meps.ahrq.gov/data_stats/download_data_files.jsp')
# Step 2: Creating a Python Pandas DataFrame based on the .csv file
#created in the previous step 12/10/2022
colname = ['file_name']
df = pd.read_csv(r'C:/Data/MEPS_fn.csv',  sep='\t', names = colname)
df.drop(df[df['file_name'].str.contains('replaced|CD-ROM|NHC|NHEA|NHIS Link| \
HC-IC Linked')].index, inplace=True)
df["file_id"] = df["file_name"].str.extract(r"([A-Z])[A-Z]+-(\d+[A-Z]*)") \
.sum(axis=1).str.lower()
df['file_id'] = df['file_id'].str.replace('h0', 'h').str.replace('h36', 'h036') \
.str.replace('h36brr', 'h036brr')
df["url1"] =  \
"https://meps.ahrq.gov/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-" \
+ df["file_name"].str.extract(r"(\d+[A-Z]*)").sum(axis=1).astype(str)
df.reset_index(drop = True, inplace = True)
print('There are', f"{len(df)}",
'MEPS public-use filenames listed in the MEPS Data File Web Page.\n')
# Step 3: Constructing a Python Pandas DataFrame (in Excel format)
# with MEPS URLs (rows) that are specific to data file formats
#(ASCII, SAS transport, SAS V9, Stata, and Excel) 12/10/2022
url2_list = []
for item in df.index:
url1 = df['url1'][item]
response = requests.get(url1)
soup = BeautifulSoup(response.text, "html.parser")
for link in soup.find_all('a'):
if link.text.endswith('ZIP'):
domain = 'https://meps.ahrq.gov'
url2 = domain + link.get('href').strip('..')
url2_list.append(url2)
print('There are', f"{len(url2_list)}", \
'URLs that are specific to data file formats.\n')
df2  = pd.DataFrame(url2_list, columns=['url2'])
df2['file_id'] = df2['url2'].str.extract(r"([h]\d+[abcdefghir]*(?!\d))")\
.sum(axis=1)
df2['file_id'] = df2['file_id'].str.replace('da', '')
df = df.drop('url1',axis=1)
merged_df = pd.merge(df, df2, on='file_id', validate ="one_to_many")
print('There are', f"{len(merged_df)}", \
'URLs that are specific to data file formats with file description.\n')
with pd.ExcelWriter('merged_df.xlsx') as writer:
merged_df.to_excel(writer, sheet_name='data_urls', index=False)
writer.sheets['data_urls'].set_column(45, 3, 45)
import os
os.getcwd()
os.chdir(r"C:\Data")
# Step 1: Creating a csv file with MEPS data filenames listed on the web 12/10/2022
import requests
from bs4 import BeautifulSoup, re, Comment
import pandas as pd
import xlsxwriter
# find all options match the start and end string
def extractOptions(inputData):
sub1 = str(re.escape('<option value="All">All data files</option>'))
sub2 = str(re.escape('</select>'))
result = re.findall(sub1+"(.*)"+sub2, inputData, flags=re.S)
if len(result) > 0:
return result[0]
# find the actual data from each option
def extracData(inputData):
sub1 = str(re.escape('>'))
sub2 = str(re.escape('</option>'))
result =  re.findall(sub1+"(.*)"+sub2, inputData, flags=re.S)
if len(result) > 0:
return result[0]
return ''
def main(base_url):
response = requests.get(base_url)
soup = BeautifulSoup(response.text, "html.parser")
comments = soup.find_all(string=lambda text: isinstance(text, Comment))
for c in comments:
if '<select id="pufnumber" size=1 name="cboPufNumber">' in c:
options = extractOptions(c)
ops = options.splitlines() #split text into lines
fp = open(r'C:/Data/MEPS_fn.csv', 'w')
for op in ops:
data = extracData(op)
if data != '': #check if the data found
fp.write(data +'\n')
fp.close()
with open(r'C:/Data/MEPS_fn.csv', 'r') as buff:
for i, line in enumerate(buff, 1):
pass
print('There are', f"{(i)}",  \
'filenames listed in the MEPS Data File Web Page\n')
main('https://meps.ahrq.gov/data_stats/download_data_files.jsp')
quit
install.packages("ret")
reticulate::repl_python()
# Step 1: Creating a csv file with MEPS data filenames listed on the web 12/10/2022
import requests
from bs4 import BeautifulSoup, re, Comment
import pandas as pd
import xlsxwriter
# find all options match the start and end string
def extractOptions(inputData):
sub1 = str(re.escape('<option value="All">All data files</option>'))
sub2 = str(re.escape('</select>'))
result = re.findall(sub1+"(.*)"+sub2, inputData, flags=re.S)
if len(result) > 0:
return result[0]
# find the actual data from each option
def extracData(inputData):
sub1 = str(re.escape('>'))
sub2 = str(re.escape('</option>'))
result =  re.findall(sub1+"(.*)"+sub2, inputData, flags=re.S)
if len(result) > 0:
return result[0]
return ''
def main(base_url):
response = requests.get(base_url)
soup = BeautifulSoup(response.text, "html.parser")
comments = soup.find_all(string=lambda text: isinstance(text, Comment))
for c in comments:
if '<select id="pufnumber" size=1 name="cboPufNumber">' in c:
options = extractOptions(c)
ops = options.splitlines() #split text into lines
fp = open(r'C:/Data/MEPS_fn.csv', 'w')
for op in ops:
data = extracData(op)
if data != '': #check if the data found
fp.write(data +'\n')
fp.close()
with open(r'C:/Data/MEPS_fn.csv', 'r') as buff:
for i, line in enumerate(buff, 1):
pass
print('There are', f"{(i)}",  \
'filenames listed in the MEPS Data File Web Page\n')
main('https://meps.ahrq.gov/data_stats/download_data_files.jsp')
# Step 1: Creating a csv file with MEPS data filenames listed on the web 12/10/2022
import requests
from bs4 import BeautifulSoup, re, comment
import pandas as pd
import xlsxwriter
# find all options match the start and end string
def extractOptions(inputData):
sub1 = str(re.escape('<option value="All">All data files</option>'))
sub2 = str(re.escape('</select>'))
result = re.findall(sub1+"(.*)"+sub2, inputData, flags=re.S)
if len(result) > 0:
return result[0]
# find the actual data from each option
def extracData(inputData):
sub1 = str(re.escape('>'))
sub2 = str(re.escape('</option>'))
result =  re.findall(sub1+"(.*)"+sub2, inputData, flags=re.S)
if len(result) > 0:
return result[0]
return ''
def main(base_url):
response = requests.get(base_url)
soup = BeautifulSoup(response.text, "html.parser")
comments = soup.find_all(string=lambda text: isinstance(text, Comment))
for c in comments:
if '<select id="pufnumber" size=1 name="cboPufNumber">' in c:
options = extractOptions(c)
ops = options.splitlines() #split text into lines
fp = open(r'C:/Data/MEPS_fn.csv', 'w')
for op in ops:
data = extracData(op)
if data != '': #check if the data found
fp.write(data +'\n')
fp.close()
with open(r'C:/Data/MEPS_fn.csv', 'r') as buff:
for i, line in enumerate(buff, 1):
pass
print('There are', f"{(i)}",  \
'filenames listed in the MEPS Data File Web Page\n')
main('https://meps.ahrq.gov/data_stats/download_data_files.jsp')
quit
# load the SASmarkdown package
library(SASmarkdown)
# set up the options so that knit knows where you SAS executable is
# set the linesize to be easily readable on letter size paper, portrait
# and set the knitr options using opts_chunk$set().
saspath <- "C:/Program Files/SASHome/SASFoundation/9.4/sas.exe"
sasopts <- "-nosplash -linesize 75"
knitr::opts_chunk$set(engine="sashtml", engine.path=saspath,
engine.opts=sasopts, comment=NA)
# run these commands to convince yourself that
# within this knitr session the engine changed.
knitr::opts_chunk$get()$engine
knitr::opts_chunk$get()$engine.path
knitr::opts_chunk$get()$engine.opts
knitr::opts_chunk$get()$engine.path
# load the SASmarkdown package
library(SASmarkdown)
# set up the options so that knit knows where you SAS executable is
# set and set the knitr options using opts_chunk$set().
saspath <- "C:/Program Files/SASHome/SASFoundation/9.4/sas.exe"
sasopts <- "-nosplash -linesize 75"
knitr::opts_chunk$set(engine="sashtml", engine.path=saspath,
engine.opts=sasopts, comment=NA)
knitr::opts_chunk$get()$engine
knitr::opts_chunk$get()$engine.path
knitr::opts_chunk$get()$engine.opts
# load the SASmarkdown package
library(SASmarkdown)
# set up the options so that knit knows where you SAS executable is
# set and set the knitr options using opts_chunk$set().
saspath <- "C:/Program Files/SASHome/SASFoundation/9.4/sas.exe"
sasopts <- "-nosplash -linesize 75"
knitr::opts_chunk$set(engine="sashtml", engine.path=saspath,
engine.opts=sasopts, comment=NA)
knitr::opts_chunk$get()$engine
knitr::opts_chunk$get()$engine.path
knitr::opts_chunk$get()$engine.opts
library("dplyr")
setwd("C:/SASCourse/Week3/SAS_Codes")
load("class_r.Rdata")
class <- class_r
#print(object())
#print(ls())
class$sex <- factor(class$sex, level=c('M', 'F'),
label=c('male', 'female')
)
head(class)
library("dplyr")
setwd("C:/SASCourse/Week3/SAS_Codes")
load("class_r.Rdata")
#class <- class_r
#print(object())
#print(ls())
class_r$sex <- factor(class_r$sex, level=c('M', 'F'),
label=c('male', 'female')
)
head(class_r)
library("dplyr")
setwd("C:/SASCourse/Week3/SAS_Codes")
load("class_r.Rdata")
ls()
#class <- class_r
#print(object())
#print(ls())
class_r$sex <- factor(class_r$sex, level=c('M', 'F'),
label=c('male', 'female')
)
head(class_r)
library("dplyr")
setwd("C:/SASCourse/Week3/SAS_Codes")
load("class_r.Rdata")
ls()
#class <- class_r
#print(object())
#print(ls())
#class_r$sex <- factor(class_r$sex, level=c('M', 'F'),
#                    label=c('male', 'female')
#                   )
#head(class_r)
